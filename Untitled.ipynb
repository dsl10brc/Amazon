{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"feature_extraction.py\n",
    "\n",
    "Create the requested datasets.\n",
    "\n",
    "Author: Paul Duan <email@paulduan.com>\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import logging\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from external import greedy, ben\n",
    "from data import save_dataset\n",
    "from ml import get_dataset\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "subformatter = logging.Formatter(\"[%(asctime)s] %(levelname)s\\t> %(message)s\")\n",
    "\n",
    "COLNAMES = [\"resource\", \"manager\", \"role1\", \"role2\", \"department\",\n",
    "            \"title\", \"family_desc\", \"family\"]\n",
    "SELECTED_COLUMNS = [0, 1, 4, 5, 6, 7]\n",
    "\n",
    "EXTERNAL_DATASETS = {\n",
    "    \"greedy\": greedy,\n",
    "    \"greedy2\": greedy,\n",
    "    \"greedy3\": greedy,\n",
    "    \"bsfeats\": ben\n",
    "}\n",
    "\n",
    "\n",
    "def sparsify(X, X_test):\n",
    "    \"\"\"Return One-Hot encoded datasets.\"\"\"\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(np.vstack((X, X_test)))\n",
    "    return enc.transform(X), enc.transform(X_test)\n",
    "\n",
    "\n",
    "def create_datasets(X, X_test, y, datasets=[], use_cache=True):\n",
    "    \"\"\"\n",
    "    Generate datasets as needed with different sets of features\n",
    "    and save them to disk.\n",
    "    The datasets are created by combining a base feature set (combinations of\n",
    "    the original variables) with extracted feature sets, with some additional\n",
    "    variants.\n",
    "\n",
    "    The nomenclature is as follows:\n",
    "    Base datasets:\n",
    "        - basic: the original columns, minus role1, role2, and role_code\n",
    "        - tuples: all order 2 combinations of the original columns\n",
    "        - triples: all order 3 combinations of the original columns\n",
    "        - greedy[1,2,3]: three different datasets obtained by performing\n",
    "            greedy feature selection with different seeds on the triples\n",
    "            dataset\n",
    "        - effects: experimental. Created to try out a suggestion by Gxav\n",
    "            after the competition\n",
    "\n",
    "    Feature sets and variants:\n",
    "    (denoted by the letters after the underscore in the base dataset name):\n",
    "        - s: the base dataset has been sparsified using One-Hot encoding\n",
    "        - c: the rare features have been consolidated into one category\n",
    "        - f: extracted features have been appended, with a different set for\n",
    "            linear models than for tree-based models\n",
    "        - b: Benjamin's extracted features.\n",
    "        - d: interactions for the extracted feature set have been added\n",
    "        - l: the extracted features have been log transformed\n",
    "    \"\"\"\n",
    "    if use_cache:\n",
    "        # Check if all files exist. If not, generate the missing ones\n",
    "        DATASETS = []\n",
    "        for dataset in datasets:\n",
    "            try:\n",
    "                with open(\"cache/%s.pkl\" % dataset, 'rb'):\n",
    "                    pass\n",
    "            except IOError:\n",
    "                logger.warning(\"couldn't load dataset %s, will generate it\",\n",
    "                               dataset)\n",
    "                DATASETS.append(dataset.split('_')[0])\n",
    "    else:\n",
    "        DATASETS = [\"basic\", \"tuples\", \"triples\",\n",
    "                    \"greedy\", \"greedy2\", \"greedy3\"]\n",
    "\n",
    "    # Datasets that require external code to be generated\n",
    "    for dataset, module in EXTERNAL_DATASETS.iteritems():\n",
    "        if not get_dataset(dataset):\n",
    "            module.create_features()\n",
    "\n",
    "    # Generate the missing datasets\n",
    "    if len(DATASETS):\n",
    "        bsfeats, bsfeats_test = get_dataset('bsfeats')\n",
    "\n",
    "        basefeats, basefeats_test = create_features(X, X_test, 3)\n",
    "        save_dataset(\"base_feats\", basefeats, basefeats_test)\n",
    "\n",
    "        lrfeats, lrfeats_test = pre_process(*create_features(X, X_test, 0))\n",
    "        save_dataset(\"lrfeats\", lrfeats, lrfeats_test)\n",
    "\n",
    "        feats, feats_test = pre_process(*create_features(X, X_test, 1))\n",
    "        save_dataset(\"features\", feats, feats_test)\n",
    "\n",
    "        meta, meta_test = pre_process(*create_features(X, X_test, 2),\n",
    "                                      normalize=False)\n",
    "        save_dataset(\"metafeatures\", meta, meta_test)\n",
    "\n",
    "        X = X[:, SELECTED_COLUMNS]\n",
    "        X_test = X_test[:, SELECTED_COLUMNS]\n",
    "        save_dataset(\"basic\", X, X_test)\n",
    "\n",
    "        Xt = create_tuples(X)\n",
    "        Xt_test = create_tuples(X_test)\n",
    "        save_dataset(\"tuples\", Xt, Xt_test)\n",
    "\n",
    "        Xtr = create_tuples(X)\n",
    "        Xtr_test = create_tuples(X_test)\n",
    "        save_dataset(\"triples\", Xtr, Xtr_test)\n",
    "\n",
    "        Xe, Xe_test = create_effects(X, X_test, y)\n",
    "        save_dataset(\"effects\", Xe, Xe_test)\n",
    "\n",
    "        feats_d, feats_d_test = pre_process(basefeats, basefeats_test,\n",
    "                                            create_divs=True)\n",
    "        bsfeats_d, bsfeats_d_test = pre_process(bsfeats, bsfeats_test,\n",
    "                                                create_divs=True)\n",
    "        feats_l, feats_l_test = pre_process(basefeats, basefeats_test,\n",
    "                                            log_transform=True)\n",
    "        lrfeats_l, lrfeats_l_test = pre_process(lrfeats, lrfeats_test,\n",
    "                                                log_transform=True)\n",
    "        bsfeats_l, bsfeats_l_test = pre_process(bsfeats, bsfeats_test,\n",
    "                                                log_transform=True)\n",
    "\n",
    "        for ds in DATASETS:\n",
    "            Xg, Xg_test = get_dataset(ds)\n",
    "            save_dataset(ds + '_b', Xg, Xg_test, bsfeats, bsfeats_test)\n",
    "            save_dataset(ds + '_f', Xg, Xg_test, feats, feats_test)\n",
    "            save_dataset(ds + '_fd', Xg, Xg_test, feats_d, feats_d_test)\n",
    "            save_dataset(ds + '_bd', Xg, Xg_test, bsfeats_d, bsfeats_d_test)\n",
    "            Xs, Xs_test = sparsify(Xg, Xg_test)\n",
    "            save_dataset(ds + '_sf', Xs, Xs_test, lrfeats, lrfeats_test)\n",
    "            save_dataset(ds + '_sfl', Xs, Xs_test, lrfeats_l, lrfeats_l_test)\n",
    "            save_dataset(ds + '_sfd', Xs, Xs_test, feats_d, feats_d_test)\n",
    "            save_dataset(ds + '_sb', Xs, Xs_test, bsfeats, bsfeats_test)\n",
    "            save_dataset(ds + '_sbl', Xs, Xs_test, bsfeats_l, bsfeats_l_test)\n",
    "            save_dataset(ds + '_sbd', Xs, Xs_test, bsfeats_d, bsfeats_d_test)\n",
    "\n",
    "            if issubclass(Xg.dtype.type, np.integer):\n",
    "                consolidate(Xg, Xg_test)\n",
    "                save_dataset(ds + '_c', Xg, Xg_test)\n",
    "                save_dataset(ds + '_cf', Xg, Xg_test, feats, feats_test)\n",
    "                save_dataset(ds + '_cb', Xg, Xg_test, bsfeats, bsfeats_test)\n",
    "                Xs, Xs_test = sparsify(Xg, Xg_test)\n",
    "                save_dataset(ds + '_sc', Xs, Xs_test)\n",
    "                save_dataset(ds + '_scf', Xs, Xs_test, feats, feats_test)\n",
    "                save_dataset(ds + '_scfl', Xs, Xs_test, feats_l, feats_l_test)\n",
    "                save_dataset(ds + '_scb', Xs, Xs_test, bsfeats, bsfeats_test)\n",
    "                save_dataset(ds + '_scbl', Xs, Xs_test,\n",
    "                             bsfeats_l, bsfeats_l_test)\n",
    "\n",
    "\n",
    "def create_effects(X_train, X_test, y):\n",
    "    \"\"\"\n",
    "    Create a dataset where the features are the effects of a\n",
    "    logistic regression trained on sparsified data.\n",
    "    This has been added post-deadline after talking with Gxav.\n",
    "    \"\"\"\n",
    "    from sklearn import linear_model, cross_validation\n",
    "    from itertools import izip\n",
    "    Xe_train = np.zeros(X_train.shape)\n",
    "    Xe_test = np.zeros(X_test.shape)\n",
    "    n_cols = Xe_train.shape[1]\n",
    "\n",
    "    model = linear_model.LogisticRegression(C=2)\n",
    "    X_train, X_test = sparsify(X_train, X_test)\n",
    "\n",
    "    kfold = cross_validation.KFold(len(y), 5)\n",
    "    for train, cv in kfold:\n",
    "        model.fit(X_train[train], y[train])\n",
    "        colindices = X_test.nonzero()[1]\n",
    "        for i, k in izip(cv, range(len(cv))):\n",
    "            for j in range(n_cols):\n",
    "                z = colindices[n_cols*k + j]\n",
    "                Xe_train[i, j] = model.coef_[0, z]\n",
    "\n",
    "    model.fit(X_train, y)\n",
    "    colindices = X_test.nonzero()[1]\n",
    "    for i in range(Xe_test.shape[0]):\n",
    "        for j in range(n_cols):\n",
    "            z = colindices[n_cols*i + j]\n",
    "            Xe_test[i, j] = model.coef_[0, z]\n",
    "\n",
    "    return Xe_train, Xe_test\n",
    "\n",
    "\n",
    "def create_features(X_train, X_test, feature_set=0):\n",
    "    \"\"\"\n",
    "    Extract features from the training and test set.\n",
    "    Each feature set is defined as a list of lambda functions.\n",
    "    \"\"\"\n",
    "    logger.info(\"performing feature extraction (feature_set=%d)\", feature_set)\n",
    "    features_train = []\n",
    "    features_test = []\n",
    "    dictionaries = get_pivottable(X_train, X_test)\n",
    "    dictionaries_train = get_pivottable(X_train, X_test, use='train')\n",
    "    dictionaries_test = get_pivottable(X_test, X_test, use='test')\n",
    "\n",
    "    # 0: resource, 1: manager, 2: role1, 3: role2, 4: department,\n",
    "    # 5: title, 6: family_desc, 7: family\n",
    "    feature_lists = [\n",
    "        [  # 0: LR features\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[0]].get(row[0], 0) if j > 0 and j < 7 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[1]].get(row[1], 0) if j > 1 and j < 7 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[2]].get(row[2], 0) if j > 2 and j < 7 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[3]].get(row[3], 0) if j > 3 and j < 7 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[4]].get(row[4], 0) if j > 4 and j < 7 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[5]].get(row[5], 0) if j > 5 and j < 7 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[6]].get(row[6], 0) if j > 6 and j < 7 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[7]].get(row[7], 0) if j > 7 and j < 7 else 0,\n",
    "\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[0]].get(row[0], 0)**2 if j in range(7) else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[j]].get(row[0], 0)/x['total']\n",
    "            if j > 0 and j < 7 else 0,\n",
    "\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[j]].get(row[j], 0)/len(x[COLNAMES[j]].values()),\n",
    "\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[j]].get(row[j], 0) / dictionaries[j]['total'],\n",
    "\n",
    "            lambda x, row, j:\n",
    "            math.log(x[COLNAMES[0]].get(row[0], 0)) if j in range(5) else 0,\n",
    "\n",
    "            lambda x, row, j:\n",
    "            int(row[j] not in dictionaries_train[j]),\n",
    "\n",
    "            lambda x, row, j:\n",
    "            int(row[j] not in dictionaries_test[j]),\n",
    "        ],\n",
    "\n",
    "        [  # 1: Tree features\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[0]].get(row[0], 0),\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[1]].get(row[1], 0),\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[2]].get(row[2], 0),\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[3]].get(row[3], 0),\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[4]].get(row[4], 0),\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[5]].get(row[5], 0),\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[6]].get(row[6], 0),\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[7]].get(row[7], 0),\n",
    "\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[j]].get(row[0], 0)/x['total'] if j > 0 else 0,\n",
    "        ],\n",
    "\n",
    "        [  # 2: Metafeatures\n",
    "            lambda x, row, j:\n",
    "            dictionaries_train[j].get(row[j], {}).get('total', 0),\n",
    "            lambda x, row, j:\n",
    "            dictionaries_train[j].get(row[j], {}).get('total', 0) == 0,\n",
    "        ],\n",
    "\n",
    "        [  # 3: Base features\n",
    "            lambda x, row, j:\n",
    "            x['total'] if j == 0 else 0,\n",
    "\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[0]].get(row[0], 0) if j > 0 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[1]].get(row[1], 0) if j > 1 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[2]].get(row[2], 0) if j > 2 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[3]].get(row[3], 0) if j > 3 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[4]].get(row[4], 0) if j > 4 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[5]].get(row[5], 0) if j > 5 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[6]].get(row[6], 0) if j > 6 else 0,\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[7]].get(row[7], 0) if j > 7 else 0,\n",
    "\n",
    "            lambda x, row, j:\n",
    "            x[COLNAMES[0]].get(row[0], 0)**2 if j in range(8) else 0,\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    feature_generator = feature_lists[feature_set]\n",
    "\n",
    "    # create feature vectors\n",
    "    logger.debug(\"creating feature vectors\")\n",
    "    features_train = []\n",
    "    for row in X_train:\n",
    "        features_train.append([])\n",
    "        for j in range(len(COLNAMES)):\n",
    "            for feature in feature_generator:\n",
    "                feature_row = feature(dictionaries[j][row[j]], row, j)\n",
    "                features_train[-1].append(feature_row)\n",
    "    features_train = np.array(features_train)\n",
    "\n",
    "    features_test = []\n",
    "    for row in X_test:\n",
    "        features_test.append([])\n",
    "        for j in range(len(COLNAMES)):\n",
    "            for feature in feature_generator:\n",
    "                feature_row = feature(dictionaries[j][row[j]], row, j)\n",
    "                features_test[-1].append(feature_row)\n",
    "    features_test = np.array(features_test)\n",
    "\n",
    "    return features_train, features_test\n",
    "\n",
    "\n",
    "def pre_process(features_train, features_test,\n",
    "                create_divs=False, log_transform=False, normalize=True):\n",
    "    \"\"\"\n",
    "    Take lists of feature columns as input, pre-process them (eventually\n",
    "    performing some transformation), then return nicely formatted numpy arrays.\n",
    "    \"\"\"\n",
    "    logger.info(\"performing preprocessing\")\n",
    "\n",
    "    features_train = list(features_train.T)\n",
    "    features_test = list(features_test.T)\n",
    "    features_train = [list(feature) for feature in features_train]\n",
    "    features_test = [list(feature) for feature in features_test]\n",
    "\n",
    "    # remove constant features\n",
    "    for i in range(len(features_train) - 1, -1, -1):\n",
    "        if np.var(features_train[i]) + np.var(features_test[i]) == 0:\n",
    "            features_train.pop(i)\n",
    "            features_test.pop(i)\n",
    "    n_features = len(features_train)\n",
    "\n",
    "    # create some polynomial features\n",
    "    if create_divs:\n",
    "        for i in range(n_features):\n",
    "            for j in range(1):\n",
    "                features_train.append([round(a/(b + 1), 3) for a, b in zip(\n",
    "                    features_train[i], features_train[j])])\n",
    "                features_test.append([round(a/(b + 1), 3) for a, b in zip(\n",
    "                    features_test[i], features_test[j])])\n",
    "\n",
    "                features_train.append([round(a/(b + 1), 3) for a, b in zip(\n",
    "                    features_train[j], features_train[i])])\n",
    "                features_test.append([round(a/(b + 1), 3) for a, b in zip(\n",
    "                    features_test[j], features_test[i])])\n",
    "\n",
    "                features_train.append([a*b for a, b in zip(\n",
    "                    features_train[j], features_train[i])])\n",
    "                features_test.append([a*b for a, b in zip(\n",
    "                    features_test[j], features_test[i])])\n",
    "\n",
    "    if log_transform:\n",
    "        tmp_train = []\n",
    "        tmp_test = []\n",
    "        for i in range(n_features):\n",
    "            tmp_train.append([math.log(a + 1) if (a + 1) > 0 else 0\n",
    "                             for a in features_train[i]])\n",
    "            tmp_test.append([math.log(a + 1) if (a + 1) > 0 else 0\n",
    "                             for a in features_test[i]])\n",
    "\n",
    "            tmp_train.append([a**2 for a in features_train[i]])\n",
    "            tmp_test.append([a**2 for a in features_test[i]])\n",
    "            tmp_train.append([a**3 for a in features_train[i]])\n",
    "            tmp_test.append([a**3 for a in features_test[i]])\n",
    "        features_train = tmp_train\n",
    "        features_test = tmp_test\n",
    "\n",
    "    logger.info(\"created %d features\", len(features_train))\n",
    "    features_train = np.array(features_train).T\n",
    "    features_test = np.array(features_test).T\n",
    "\n",
    "    # normalize the new features\n",
    "    if normalize:\n",
    "        normalizer = preprocessing.StandardScaler()\n",
    "        normalizer.fit(features_train)\n",
    "        features_train = normalizer.transform(features_train)\n",
    "        features_test = normalizer.transform(features_test)\n",
    "\n",
    "    return features_train, features_test\n",
    "\n",
    "\n",
    "def get_pivottable(X_train, X_test, use='all'):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries, one per feature in the\n",
    "    basic data, containing cross-tabulated counts\n",
    "    for each column and each value of the feature.\n",
    "    \"\"\"\n",
    "    dictionaries = []\n",
    "    if use == 'all':\n",
    "        X = np.vstack((X_train, X_test))\n",
    "        filename = \"pivottable\"\n",
    "    elif use == 'train':\n",
    "        X = X_train\n",
    "        filename = \"pivottable_train\"\n",
    "    else:\n",
    "        X = X_test\n",
    "        filename = \"pivottable_test\"\n",
    "\n",
    "    for i in range(len(COLNAMES)):\n",
    "        dictionaries.append({'total': 0})\n",
    "\n",
    "    try:\n",
    "        with open(\"cache/%s.pkl\" % filename, 'rb') as f:\n",
    "            logger.debug(\"loading cross-tabulated data from cache\")\n",
    "            dictionaries = pickle.load(f)\n",
    "    except IOError:\n",
    "        logger.debug(\"no cache found, cross-tabulating data\")\n",
    "        for i, row in enumerate(X):\n",
    "            for j in range(len(COLNAMES)):\n",
    "                dictionaries[j]['total'] += 1\n",
    "                if row[j] not in dictionaries[j]:\n",
    "                    dictionaries[j][row[j]] = {'total': 1}\n",
    "                    for k, key in enumerate(COLNAMES):\n",
    "                        dictionaries[j][row[j]][key] = {row[k]: 1}\n",
    "                else:\n",
    "                    dictionaries[j][row[j]]['total'] += 1\n",
    "                    for k, key in enumerate(COLNAMES):\n",
    "                        if row[k] not in dictionaries[j][row[j]][key]:\n",
    "                            dictionaries[j][row[j]][key][row[k]] = 1\n",
    "                        else:\n",
    "                            dictionaries[j][row[j]][key][row[k]] += 1\n",
    "        with open(\"cache/%s.pkl\" % filename, 'wb') as f:\n",
    "            pickle.dump(dictionaries, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return dictionaries\n",
    "\n",
    "\n",
    "def create_tuples(X):\n",
    "    logger.debug(\"creating feature tuples\")\n",
    "    cols = []\n",
    "    for i in range(X.shape[1]):\n",
    "        for j in range(i, X.shape[1]):\n",
    "            cols.append(X[:, i] + X[:, j]*3571)\n",
    "    return np.hstack((X, np.vstack(cols).T))\n",
    "\n",
    "\n",
    "def create_triples(X):\n",
    "    logger.debug(\"creating feature triples\")\n",
    "    cols = []\n",
    "    for i in range(X.shape[1]):\n",
    "        for j in range(i, X.shape[1]):\n",
    "            for k in range(j, X.shape[1]):\n",
    "                cols.append(X[:, i]*3461 + X[:, j]*5483 + X[:, k])\n",
    "    return np.hstack((X, np.vstack(cols).T))\n",
    "\n",
    "\n",
    "def consolidate(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Transform in-place the given dataset by consolidating\n",
    "    rare features into a single category.\n",
    "    \"\"\"\n",
    "    X = np.vstack((X_train, X_test))\n",
    "    relabeler = preprocessing.LabelEncoder()\n",
    "\n",
    "    for j in range(X.shape[1]):\n",
    "        relabeler.fit(X[:, j])\n",
    "        X[:, j] = relabeler.transform(X[:, j])\n",
    "        X_train[:, j] = relabeler.transform(X_train[:, j])\n",
    "        X_test[:, j] = relabeler.transform(X_test[:, j])\n",
    "\n",
    "        raw_counts = np.bincount(X[:, j])\n",
    "        indices = np.nonzero(raw_counts)[0]\n",
    "        counts = dict((x, raw_counts[x]) for x in indices)\n",
    "        max_value = np.max(X[:, j])\n",
    "\n",
    "        for i in range(X_train.shape[0]):\n",
    "            if counts[X_train[i, j]] <= 1:\n",
    "                X_train[i, j] = max_value + 1\n",
    "\n",
    "        for i in range(X_test.shape[0]):\n",
    "            if counts[X_test[i, j]] <= 1:\n",
    "                X_test[i, j] = max_value + 1\n",
    "\n",
    "\n",
    "class OneHotEncoder():\n",
    "    \"\"\"\n",
    "    OneHotEncoder takes data matrix with categorical columns and\n",
    "    converts it to a sparse binary matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.keymap = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.keymap = []\n",
    "        for col in X.T:\n",
    "            uniques = set(list(col))\n",
    "            self.keymap.append(dict((key, i) for i, key in enumerate(uniques)))\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.keymap is None:\n",
    "            self.fit(X)\n",
    "\n",
    "        outdat = []\n",
    "        for i, col in enumerate(X.T):\n",
    "            km = self.keymap[i]\n",
    "            num_labels = len(km)\n",
    "            spmat = sparse.lil_matrix((X.shape[0], num_labels))\n",
    "            for j, val in enumerate(col):\n",
    "                if val in km:\n",
    "                    spmat[j, km[val]] = 1\n",
    "            outdat.append(spmat)\n",
    "        outdat = sparse.hstack(outdat).tocsr()\n",
    "        return outdat\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
